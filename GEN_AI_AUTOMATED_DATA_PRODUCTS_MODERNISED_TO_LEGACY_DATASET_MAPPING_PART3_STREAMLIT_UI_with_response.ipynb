{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "150ede34-08fc-4930-a4b1-09cc76226df4",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "This code creates a Streamlit web application for comparing and mapping data between modernized and legacy tables in Snowflake. It uses AI models to analyze the data and generate mappings, with options for incorporating vector embeddings and sample data. The app allows users to input table information, select AI models and parameters, and view the generated prompts and AI responses.\n\n# Import necessary libraries\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nimport json\n\n# Get the current Snowflake session\nsession = get_active_session()\n\n# Set up the Streamlit app title\nst.title(\"Data Comparison and Mapping Tool\")\n\n# Initialize session state variables for storing prompt, AI response, and token count\n# These persist across reruns of the app\n\n# Input fields for user to enter database, schema, and table information\n# Default values are provided for convenience\n\n# Dropdown for selecting the AI model to use\n\n# Checkboxes for additional options (vector embeddings and sample data)\n\n# Function to fetch available vector embeddings based on table names\ndef fetch_available_embeddings(modern_table, legacy_table):\n    # Query Snowflake to get relevant vector mappings\n\n# Fetch and display available embeddings if any are found\n\n# Function to generate the prompt for the AI model\ndef generate_prompt():\n    # Construct the prompt based on user selections and data\n    # Include vector embeddings and/or sample data if selected\n    # Format the prompt with instructions for the AI model\n\n# Function to get the AI response using Snowflake's CORTEX.COMPLETE function\ndef get_ai_response(options):\n    # Call the AI model with the generated prompt and user-specified parameters\n    # Handle the response and any potential errors\n\n# Button to generate the prompt\n# When clicked, it generates the prompt and calculates the token count\n\n# Display the generated prompt and token count if available\n\n# UI for setting AI model parameters (temperature, max tokens, top p)\n\n# Button to get the AI response\n# When clicked, it calls the AI model with the generated prompt and chosen parameters\n\n# Display the AI model response, including the generated text and usage statistics"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": false
   },
   "source": "import streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nimport json\n\n# Get the current Snowflake session\nsession = get_active_session()\n\nst.title(\"Data Comparison and Mapping Tool\")\n\n# Initialize session state variables\nif 'prompt' not in st.session_state:\n    st.session_state.prompt = \"\"\nif 'ai_response' not in st.session_state:\n    st.session_state.ai_response = \"\"\nif 'token_count' not in st.session_state:\n    st.session_state.token_count = 0\n\n# Input fields for database, schema, and table names\ndatabase_name = st.text_input(\"Enter Database Name\", \"DATA_OPS_MAPPING\")\nschema_name = st.text_input(\"Enter Schema Name\", \"DATA_ONESTREAM\")\nmodern_table_name = st.text_input(\"Enter Modernised Table Name\", \"Deposit_Accounts_Onestream\")\nlegacy_table_name = st.text_input(\"Enter Legacy Table Name\", \"LEGACY_DEPOSIT_ACCOUNTS\")\nrecord_limit = st.number_input(\"Enter Sample Record Limit\", min_value=1, value=5)\n\n# Model selection\nmodel = st.selectbox(\"Select AI Model\", [\n    \"snowflake-arctic\", \"mistral-large\", \"reka-flash\", \"reka-core\", \"mixtral-8x7b\",\n    \"jamba-instruct\", \"llama2-70b-chat\", \"llama3-8b\", \"llama3-70b\", \"llama3.1-8b\",\n    \"llama3.1-70b\", \"llama3.1-405b\", \"mistral-7b\", \"gemma-7b\"\n])\n\n# Options for prompt generation\nuse_vector_embeddings = st.checkbox(\"Use Vector Embeddings\")\nuse_sample_data = st.checkbox(\"Use Sample Data\")\n\ndef fetch_available_embeddings(modern_table, legacy_table):\n    query = f\"\"\"\n    SELECT VECTOR_MAPPING_FOR_TABLE_NAME \n    FROM SMART_AI_MAPPER.SMART_AI_MAPPER_TOOL.TOP_3_SIMILAR_FIELDS_FROM_VEC_EMB\n    WHERE UPPER(VECTOR_MAPPING_FOR_TABLE_NAME) LIKE '%{modern_table.upper()}%' \n    AND UPPER(VECTOR_MAPPING_FOR_TABLE_NAME) LIKE '%{legacy_table.upper()}%'\n    GROUP BY 1\n    \"\"\"\n    result = session.sql(query).collect()\n    return [row['VECTOR_MAPPING_FOR_TABLE_NAME'] for row in result]\n\n# Fetch available embeddings\navailable_embeddings = fetch_available_embeddings(modern_table_name, legacy_table_name)\n\n# Dropdown for selecting embedding\nselected_embedding = None\nif available_embeddings:\n    selected_embedding = st.selectbox(\"Select Vector Embedding\", available_embeddings)\nelse:\n    st.warning(\"No matching vector embeddings found for the given table names.\")\n\ndef generate_prompt():\n    combined_data = {}\n    prompt_text = \"Given the provided data, compare the fields between the Modernised and Legacy tables. \"\n    \n    if use_vector_embeddings and selected_embedding:\n        # Fetch vector embeddings and mappings\n        vector_mappings = session.sql(f\"\"\"\n        SELECT ARRAY_AGG(\n            OBJECT_CONSTRUCT(*)) AS JSON_DATA FROM\n            (\n        SELECT VECTOR_MAPPING_FOR_TABLE_NAME,        \n         LISTAGG(CONCAT(MODERNISED_TABLE_FIELD_NAME,'-',TOP_SIMILARITIES_LEGACY_FIELDS,',')) within group \n            (ORDER BY MODERNISED_TABLE_FIELD_NAME ) AS Modern_table_field_to_legacy_table_mappings\n        FROM  SMART_AI_MAPPER.SMART_AI_MAPPER_TOOL.TOP_3_SIMILAR_FIELDS_FROM_VEC_EMB\n        WHERE VECTOR_MAPPING_FOR_TABLE_NAME = '{selected_embedding}'\n        GROUP BY 1)\n        \"\"\").collect()[0]['JSON_DATA']\n        combined_data[\"vector_mappings\"] = vector_mappings\n        prompt_text += \"Use the vector embeddings to help map fields between the Modernised table and the Legacy table. \"\n    \n    if use_sample_data:\n        # Call the stored procedure for modern table\n        modern_result = session.call('fetch_and_process_table_data', \n                                     database_name, \n                                     schema_name, \n                                     modern_table_name, \n                                     record_limit)\n        \n        # Call the stored procedure for legacy table\n        legacy_result = session.call('fetch_and_process_table_data', \n                                     database_name, \n                                     schema_name, \n                                     legacy_table_name, \n                                     record_limit)\n        \n        combined_data[\"modern_data\"] = modern_result\n        combined_data[\"legacy_data\"] = legacy_result\n        prompt_text += \"Analyze the sample data to identify discrepancies and provide a mapping between the two tables. Highlight any differences in field names or data values. \"\n    \n    prompt_text += \"\"\"\n    For fields with mismatches, suggest corrections and as per mapping identified, provide a Snowflake compatiable SQL query to transform data from Modernised table to align with the Legacy Table. \n    Ensure that all data matches correctly between Modernised and Legacy, Report if any discrepancies and apply if transformations required in the snowflake \n    SQL generated.\n    If there are fields in one table that do not have direct matches in the other table, note these discrepancies and indicate how to handle them.\n    \n    Data: {data}\n    \"\"\"\n    \n    st.session_state.prompt = prompt_text.format(data=json.dumps(combined_data))\n\ndef get_ai_response(options):\n    try:\n        # Call the CORTEX.COMPLETE function directly\n        result = session.sql(f\"\"\"\n        SELECT SNOWFLAKE.CORTEX.COMPLETE(\n            '{model}',\n            ARRAY_CONSTRUCT(OBJECT_CONSTRUCT('role', 'user', 'content', '{st.session_state.prompt.replace(\"'\", \"''\")}')),\n            OBJECT_CONSTRUCT('temperature', {options['temperature']}, 'max_tokens', {options['max_tokens']}, 'top_p', {options['top_p']})\n        ) AS response\n        \"\"\").collect()\n        \n        if result and len(result) > 0:\n            response_json = json.loads(result[0]['RESPONSE'])\n            st.session_state.ai_response = response_json\n        else:\n            st.error(\"No response received from the AI model.\")\n    except Exception as e:\n        st.error(f\"An error occurred: {str(e)}\")\n        st.session_state.ai_response = \"Error: Unable to get AI response.\"\n\nif st.button(\"Generate Prompt\"):\n    generate_prompt()\n    \n    # Calculate token count\n    try:\n        token_count_result = session.sql(f\"\"\"\n        SELECT SNOWFLAKE.CORTEX.COUNT_TOKENS(\n            '{model}',\n            '{st.session_state.prompt.replace(\"'\", \"''\")}'\n        ) AS token_count\n        \"\"\").collect()\n        \n        if token_count_result and len(token_count_result) > 0:\n            st.session_state.token_count = token_count_result[0]['TOKEN_COUNT']\n        else:\n            st.warning(\"Unable to calculate token count.\")\n    except Exception as e:\n        st.warning(f\"Error calculating token count: {str(e)}\")\n\nif st.session_state.prompt:\n    st.subheader(\"Generated Prompt:\")\n    st.text_area(\"Prompt\", st.session_state.prompt, height=300)\n    st.write(f\"Token Count: {st.session_state.token_count}\")\n    \n    st.subheader(\"Model Parameters\")\n    temperature = st.slider(\"Temperature\", 0.0, 1.0, 0.7, 0.1)\n    max_tokens = st.number_input(\"Max Tokens\", 1, 4096, min(st.session_state.token_count, 4096))\n    top_p = st.slider(\"Top P\", 0.0, 1.0, 1.0, 0.1)\n    \n    if st.button(\"Get AI Response\"):\n        options = {\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n            \"top_p\": top_p,\n        }\n        get_ai_response(options)\n\nif st.session_state.ai_response:\n    st.subheader(\"AI Model Response:\")\n    if isinstance(st.session_state.ai_response, dict):\n        if 'choices' in st.session_state.ai_response and len(st.session_state.ai_response['choices']) > 0:\n            message = st.session_state.ai_response['choices'][0].get('messages', '')\n            st.markdown(message)\n        \n        if 'usage' in st.session_state.ai_response:\n            usage = st.session_state.ai_response['usage']\n            st.write(\"Token Usage:\")\n            st.write(f\"- Completion tokens: {usage.get('completion_tokens', 'N/A')}\")\n            st.write(f\"- Prompt tokens: {usage.get('prompt_tokens', 'N/A')}\")\n            st.write(f\"- Total tokens: {usage.get('total_tokens', 'N/A')}\")\n        \n        if 'model' in st.session_state.ai_response:\n            st.write(f\"Model used: {st.session_state.ai_response['model']}\")\n    else:\n        st.write(st.session_state.ai_response)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5c1c8816-2723-43b1-a72f-4e3771fee06e",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "Sample Report Generated by Llama3.1-405b with Both Vector Embeddings and Smaple Data Given.\n\nAI Model Response:\nAfter analyzing the provided data, I've identified the following field mappings between the Modernised and Legacy tables:\n\nMatching Fields:\n\nACCOUNT_BALANCE (Modernised) â†’ ACCT_BALANCE (Legacy)\nACCOUNT_NUMBER (Modernised) â†’ ACCT_NUM (Legacy)\nACCOUNT_STATUS (Modernised) â†’ ACCT_STATUS (Legacy)\nACCOUNT_TYPE (Modernised) â†’ ACCT_TYPE (Legacy)\nADDRESS (Modernised) â†’ ADDR (Legacy)\nBRANCH_ID (Modernised) â†’ BRANCH_ID (Legacy)\nCITY (Modernised) â†’ CITY (Legacy)\nCREATED_TIMESTAMP (Modernised) â†’ CREATION_TIMESTAMP (Legacy)\nCUSTOMER_ID (Modernised) â†’ CUST_ID (Legacy)\nDATE_OF_BIRTH (Modernised) â†’ DOB (Legacy)\nEMAIL (Modernised) â†’ EMAIL_ADDRESS (Legacy)\nFIRST_NAME (Modernised) â†’ F_NAME (Legacy)\nINTEREST_RATE (Modernised) â†’ INTEREST (Legacy)\nKYC_STATUS (Modernised) â†’ KYC (Legacy)\nLAST_NAME (Modernised) â†’ L_NAME (Legacy)\nLAST_TRANSACTION_DATE (Modernised) â†’ LAST_TRANS_DATE (Legacy)\nMARKETING_OPT_IN (Modernised) â†’ MARKETING_CONSENT (Legacy)\nPHONE_NUMBER (Modernised) â†’ PHONE (Legacy)\nRISK_LEVEL (Modernised) â†’ RISK (Legacy)\nSSN (Modernised) â†’ SOCIAL_SECURITY_NUMBER (Legacy)\nSTATE (Modernised) â†’ STATE (Legacy)\nZIP_CODE (Modernised) â†’ POSTAL_CODE (Legacy)\nNon-Matching Fields:\n\nEMPLOYMENT_STATUS (Legacy) - No equivalent field in Modernised table.\nMIDDLE_NAME (Legacy) - No equivalent field in Modernised table.\nNATIONALITY (Legacy) - No equivalent field in Modernised table.\nMODIFICATION_TIMESTAMP (Legacy) - While there is an UPDATED_TIMESTAMP field in the Modernised table, the values do not match.\nOPEN_DATE (Legacy) - While there is a DATE_OPENED field in the Modernised table, the values do not match.\nTransformations:\n\nTo align the Modernised table with the Legacy table, the following transformations are necessary:\n\nRename ACCOUNT_BALANCE to ACCT_BALANCE\nRename ACCOUNT_NUMBER to ACCT_NUM\nRename ACCOUNT_STATUS to ACCT_STATUS\nRename ACCOUNT_TYPE to ACCT_TYPE\nRename ADDRESS to ADDR\nRename CREATED_TIMESTAMP to CREATION_TIMESTAMP\nRename CUSTOMER_ID to CUST_ID\nRename DATE_OF_BIRTH to DOB\nRename EMAIL to EMAIL_ADDRESS\nRename FIRST_NAME to F_NAME\nRename INTEREST_RATE to INTEREST\nRename KYC_STATUS to KYC\nRename LAST_NAME to L_NAME\nRename LAST_TRANSACTION_DATE to LAST_TRANS_DATE\nRename MARKETING_OPT_IN to MARKETING_CONSENT\nRename PHONE_NUMBER to PHONE\nRename RISK_LEVEL to RISK\nRename SSN to SOCIAL_SECURITY_NUMBER\nRename STATE to STATE (no change)\nRename ZIP_CODE to POSTAL_CODE\nHere is a sample Snowflake-compatible SQL query that performs these transformations:\n\nSELECT \n  ACCOUNT_BALANCE AS ACCT_BALANCE,\n  ACCOUNT_NUMBER AS ACCT_NUM,\n  ACCOUNT_STATUS AS ACCT_STATUS,\n  ACCOUNT_TYPE AS ACCT_TYPE,\n  ADDRESS AS ADDR,\n  BRANCH_ID,\n  CITY,\n  CREATED_TIMESTAMP AS CREATION_TIMESTAMP,\n  CUSTOMER_ID AS CUST_ID,\n  DATE_OF_BIRTH AS DOB,\n  EMAIL AS EMAIL_ADDRESS,\n  FIRST_NAME AS F_NAME,\n  INTEREST_RATE AS INTEREST,\n  KYC_STATUS AS KYC,\n  LAST_NAME AS L_NAME,\n  LAST_TRANSACTION_DATE AS LAST_TRANS_DATE,\n  MARKETING_OPT_IN AS MARKETING_CONSENT,\n  PHONE_NUMBER AS PHONE,\n  RISK_LEVEL AS RISK,\n  SSN AS SOCIAL_SECURITY_NUMBER,\n  STATE,\n  ZIP_CODE AS POSTAL_CODE\nFROM \n  MODERNISED_TABLE;\n\nNote that this query assumes that the Modernised table is named MODERNISED_TABLE. You should replace this with the actual table name in your database. Additionally, this query does not handle the non-matching fields (EMPLOYMENT_STATUS, MIDDLE_NAME, NATIONALITY, MODIFICATION_TIMESTAMP, and OPEN_DATE). You will need to decide how to handle these fields based on your specific use case.\n\nToken Usage:\n\nCompletion tokens: 1131\nPrompt tokens: 4173\nTotal tokens: 5304\nModel used: llama3.1-405b"
  },
  {
   "cell_type": "markdown",
   "id": "601180d7-e8e4-4ea6-bf44-0daa2d6836ab",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "Model Response for Different Dataset with Array type compared only with sample data .\n\nBased on the provided data, I've identified the following discrepancies and mapping between the Modernised and Legacy tables:\n\nField name mismatches:\nCREATED_AT (Modernised) vs. CRT_DT (Legacy)\nCUSTOMER_SINCE (Modernised) vs. CUST_SINCE (Legacy)\nDATE_OF_BIRTH (Modernised) vs. DOB (Legacy)\nANNUAL_INCOME (Modernised) vs. ANN_INC (Legacy)\nEDUCATION_LEVEL (Modernised) vs. EDU_LVL (Legacy)\nEMPLOYMENT_STATUS (Modernised) vs. EMP_STS (Legacy)\nGENDER (Modernised) vs. GNDR (Legacy)\nHOUSEHOLD_SIZE (Modernised) vs. HHLD_SZ (Legacy)\nHOME_OWNERSHIP (Modernised) vs. HOME_OWN (Legacy)\nLEGAL_NAME (Modernised) vs. LEGAL_NM (Legacy)\nMARITAL_STATUS (Modernised) vs. MRTL_STS (Legacy)\nNET_WORTH (Modernised) vs. NET_WRTH (Legacy)\nOCCUPATION (Modernised) vs. OCCPTN (Legacy)\nPARTY_TYPE (Modernised) vs. PARTY_TYP (Legacy)\nUPDATED_AT (Modernised) vs. UPD_DT (Legacy)\nData type mismatches:\nDEMOGRAPHICS (Modernised) is an array of objects, while the Legacy table has separate fields for each demographic attribute.\nData value mismatches:\nThe PARTY_ID values do not match between the two tables.\nTo transform the Modernised table to align with the Legacy table, I suggest the following Snowflake compatible SQL query:\n\nSELECT \n    m.PARTY_ID,\n    m.CREATED_AT AS CRT_DT,\n    m.CUSTOMER_SINCE AS CUST_SINCE,\n    m.DATE_OF_BIRTH AS DOB,\n    d.ANNUAL_INCOME AS ANN_INC,\n    d.EDUCATION_LEVEL AS EDU_LVL,\n    d.EMPLOYMENT_STATUS AS EMP_STS,\n    d.GENDER AS GNDR,\n    d.HOUSEHOLD_SIZE AS HHLD_SZ,\n    d.HOME_OWNERSHIP AS HOME_OWN,\n    m.LEGAL_NAME AS LEGAL_NM,\n    d.MARITAL_STATUS AS MRTL_STS,\n    d.NET_WORTH AS NET_WRTH,\n    d.OCCUPATION AS OCCPTN,\n    m.PARTY_TYPE AS PARTY_TYP,\n    m.UPDATED_AT AS UPD_DT\nFROM \n    modern_data.processed_data m,\n    LATERAL FLATTEN(m.DEMOGRAPHICS) d\n\nThis query flattens the DEMOGRAPHICS array and joins it with the main table, mapping the fields to their corresponding Legacy table fields.\n\nNote that the PARTY_ID values still do not match between the two tables. To resolve this, you may need to perform additional data cleansing or matching steps.\n\nAdditionally, the Legacy table has separate fields for each demographic attribute, while the Modernised table has an array of objects. If you want to maintain the same data structure in the transformed table, you can modify the query to use Snowflake's OBJECT_CONSTRUCT function to create separate columns for each demographic attribute. For example:\n\nSELECT \n    m.PARTY_ID,\n    m.CREATED_AT AS CRT_DT,\n    m.CUSTOMER_SINCE AS CUST_SINCE,\n    m.DATE_OF_BIRTH AS DOB,\n    OBJECT_CONSTRUCT(\n        'ANN_INC': d.ANNUAL_INCOME,\n        'EDU_LVL': d.EDUCATION_LEVEL,\n        'EMP_STS': d.EMPLOYMENT_STATUS,\n        'GNDR': d.GENDER,\n        'HHLD_SZ': d.HOUSEHOLD_SIZE,\n        'HOME_OWN': d.HOME_OWNERSHIP,\n        'MRTL_STS': d.MARITAL_STATUS,\n        'NET_WRTH': d.NET_WORTH,\n        'OCCPTN': d.OCCUPATION\n    ) AS DEMOGRAPHICS,\n    m.LEGAL_NAME AS LEGAL_NM,\n    m.PARTY_TYPE AS PARTY_TYP,\n    m.UPDATED_AT AS UPD_DT\nFROM \n    modern_data.processed_data m,\n    LATERAL FLATTEN(m.DEMOGRAPHICS) d\n\nThis will create a DEMOGRAPHICS column with a JSON object containing the individual demographic attributes.\n\nToken Usage:\n\nCompletion tokens: 1012\nPrompt tokens: 896\nTotal tokens: 1908\nModel used: llama3.1-405b"
  },
  {
   "cell_type": "code",
   "id": "75c2dcd0-5b3b-4eb2-af7e-f04ee6b8f2e6",
   "metadata": {
    "language": "sql",
    "name": "cell5",
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE AI_MAPPING_TOOL_METADATA (\n    id INTEGER AUTOINCREMENT ,\n    user STRING,\n    role STRING,\n    database_name STRING,\n    schema_name STRING,\n    modernized_table_name STRING,\n    legacy_table_name STRING,\n    vector_embeddings_used BOOLEAN,\n    sample_data_used BOOLEAN,\n    input_prompt TEXT,\n    user_liked BOOLEAN,\n    user_feedback TEXT,\n    input_token_length INTEGER,\n    output_token_length INTEGER,\n    total_token_length INTEGER,\n    model_name VARCHAR,\n    errors TEXT,\n    output_response VARCHAR,\n    start_time NUMBER(38,0),  -- Changed from TIMESTAMP_NTZ to NUMBER\n    end_time NUMBER(38,0),    -- Changed from TIMESTAMP_NTZ to NUMBER\n   total_time FLOAT,\n    sample_input_limit INTEGER,\n    --sample_input_limit TIMESTAMP_NTZ,\n    created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n    updated_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5323a51-277f-4965-ae9c-0f6d13ff5434",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "collapsed": false
   },
   "outputs": [],
   "source": "import streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import parse_json, to_timestamp, current_timestamp, lit, col, when\nfrom snowflake.snowpark.types import StringType, BooleanType, IntegerType, FloatType, TimestampType, VariantType\nimport json\nimport traceback\nfrom snowflake.snowpark.functions import current_timestamp, col, when\nfrom datetime import datetime, date\nimport json\n\n# Get the current Snowflake session\nsession = get_active_session()\n\nst.title(\"Data Comparison and Mapping Tool\")\n\n# Initialize session state variables\nif 'prompt' not in st.session_state:\n    st.session_state.prompt = \"\"\nif 'ai_response' not in st.session_state:\n    st.session_state.ai_response = \"\"\nif 'token_count' not in st.session_state:\n    st.session_state.token_count = 0\nif 'start_time' not in st.session_state:\n    st.session_state.start_time = None\nif 'end_time' not in st.session_state:\n    st.session_state.end_time = None\nif 'errors' not in st.session_state:\n    st.session_state.errors = []\nif 'metadata_id' not in st.session_state:\n    st.session_state.metadata_id = None\n\n# Input fields for database, schema, and table names\ndatabase_name = st.text_input(\"Enter Database Name\", \"SMART_AI_MAPPER\")\nschema_name = st.text_input(\"Enter Schema Name\", \"SMART_AI_MAPPER_TOOL\")\nmodern_table_name = st.text_input(\"Enter Modernised Table Name\", \"Deposit_Accounts_Modernised\")\nlegacy_table_name = st.text_input(\"Enter Legacy Table Name\", \"Deposit_Accounts_Legacy\")\nrecord_limit = st.number_input(\"Enter Sample Record Limit\", min_value=1, value=5)\n\n# Set the current database and schema\nsession.use_database(database_name)\nsession.use_schema(schema_name)\n\n# Model selection\nmodel = st.selectbox(\"Select AI Model\", [\n    \"snowflake-arctic\", \"mistral-large\", \"reka-flash\", \"reka-core\", \"mixtral-8x7b\",\n    \"jamba-instruct\", \"llama2-70b-chat\", \"llama3-8b\", \"llama3-70b\", \"llama3.1-8b\",\n    \"llama3.1-70b\", \"llama3.1-405b\", \"mistral-7b\", \"gemma-7b\"\n])\n\n# Options for prompt generation\nuse_vector_embeddings = st.checkbox(\"Use Vector Embeddings\")\nuse_sample_data = st.checkbox(\"Use Sample Data\")\n\ndef fetch_available_embeddings(modern_table, legacy_table):\n    query = f\"\"\"\n    SELECT VECTOR_MAPPING_FOR_TABLE_NAME \n    FROM TOP_3_SIMILAR_FIELDS_FROM_VEC_EMB\n    WHERE UPPER(VECTOR_MAPPING_FOR_TABLE_NAME) LIKE '%{modern_table.upper()}%' \n    AND UPPER(VECTOR_MAPPING_FOR_TABLE_NAME) LIKE '%{legacy_table.upper()}%'\n    GROUP BY 1\n    \"\"\"\n    result = session.sql(query).collect()\n    return [row['VECTOR_MAPPING_FOR_TABLE_NAME'] for row in result]\n\n# Fetch available embeddings\navailable_embeddings = fetch_available_embeddings(modern_table_name, legacy_table_name)\n\n# Dropdown for selecting embedding\nselected_embedding = None\nif available_embeddings:\n    selected_embedding = st.selectbox(\"Select Vector Embedding\", available_embeddings)\nelse:\n    st.warning(\"No matching vector embeddings found for the given table names.\")\n\ndef generate_prompt():\n    combined_data = {}\n    prompt_text = \"Given the provided data, compare the fields between the Modernised and Legacy tables. \"\n    \n    if use_vector_embeddings and selected_embedding:\n        # Fetch vector embeddings and mappings\n        vector_mappings = session.sql(f\"\"\"\n        SELECT ARRAY_AGG(\n            OBJECT_CONSTRUCT(*)) AS JSON_DATA FROM\n            (\n        SELECT VECTOR_MAPPING_FOR_TABLE_NAME,        \n         LISTAGG(CONCAT(MODERNISED_TABLE_FIELD_NAME,'-',TOP_SIMILARITIES_LEGACY_FIELDS,',')) within group \n            (ORDER BY MODERNISED_TABLE_FIELD_NAME ) AS Modern_table_field_to_legacy_table_mappings\n        FROM TOP_3_SIMILAR_FIELDS_FROM_VEC_EMB\n        WHERE VECTOR_MAPPING_FOR_TABLE_NAME = '{selected_embedding}'\n        GROUP BY 1)\n        \"\"\").collect()[0]['JSON_DATA']\n        combined_data[\"vector_mappings\"] = vector_mappings\n        prompt_text += \"Use the vector embeddings to help map fields between the Modernised table and the Legacy table. \"\n    \n    if use_sample_data:\n        # Call the stored procedure for modern table\n        modern_result = session.call('fetch_and_process_table_data', \n                                     database_name, \n                                     schema_name, \n                                     modern_table_name, \n                                     record_limit)\n        \n        # Call the stored procedure for legacy table\n        legacy_result = session.call('fetch_and_process_table_data', \n                                     database_name, \n                                     schema_name, \n                                     legacy_table_name, \n                                     record_limit)\n        \n        combined_data[\"modern_data\"] = modern_result\n        combined_data[\"legacy_data\"] = legacy_result\n        prompt_text += \"Analyze the sample data to identify discrepancies and provide a mapping between the two tables. Highlight any differences in field names or data values. \"\n    \n    prompt_text += \"\"\"\n    For fields with mismatches, suggest corrections and as per mapping identified, provide a Snowflake compatible SQL query to transform data from Modernised table to align with the Legacy Table. \n    Ensure that all data matches correctly between Modernised and Legacy, Report if any discrepancies and apply if transformations required in the snowflake \n    SQL generated.\n    If there are fields in one table that do not have direct matches in the other table, note these discrepancies and indicate how to handle them.\n    \n    Data: {data}\n    \"\"\"\n    \n    # Escape special characters in the combined_data JSON string\n    escaped_data = json.dumps(combined_data).replace('\"', '\\\\\"')\n    #escaped_data = \"Give a Response some table data in json format\"\n    st.session_state.prompt = prompt_text.format(data=escaped_data)\n\n\ndef insert_metadata():\n    current_user = session.sql(\"SELECT CURRENT_USER()\").collect()[0][0]\n    current_role = session.sql(\"SELECT CURRENT_ROLE()\").collect()[0][0]\n    \n    ai_response_json = st.session_state.ai_response\n    \n    # Extract only the \"messages\" content\n    messages_content = ai_response_json[\"choices\"][0][\"messages\"]\n    \n    # Create a new JSON object with just the \"messages\" content\n    messages = json.dumps({\"messages\": messages_content})\n    print(messages)\n    \n    # Calculate total_time\n    total_time = (st.session_state.end_time - st.session_state.start_time).total_seconds() if st.session_state.start_time and st.session_state.end_time else None\n    \n    # Truncate errors if too long\n    errors = \", \".join(st.session_state.errors) if st.session_state.errors else None\n    if errors and len(errors) > 1000:\n        errors = errors[:997] + \"...\"\n\n    # Create a DataFrame with a single row of metadata\n    metadata_df = session.create_dataframe([\n        (None, current_user, current_role, database_name, schema_name, modern_table_name, legacy_table_name,\n         use_vector_embeddings, use_sample_data, st.session_state.prompt, messages, None, None,\n         st.session_state.token_count,\n         st.session_state.ai_response.get('usage', {}).get('completion_tokens') if st.session_state.ai_response else None,\n         st.session_state.ai_response.get('usage', {}).get('total_tokens') if st.session_state.ai_response else None,\n         int(st.session_state.start_time.timestamp()) if st.session_state.start_time else None,\n         int(st.session_state.end_time.timestamp()) if st.session_state.end_time else None,\n         total_time, model, record_limit, errors, None, None)\n    ], schema=[\n        \"id\", \"user\", \"role\", \"database_name\", \"schema_name\", \"modernized_table_name\", \"legacy_table_name\",\n        \"vector_embeddings_used\", \"sample_data_used\", \"input_prompt\", \"output_response\", \"user_liked\",\n        \"user_feedback\", \"input_token_length\", \"output_token_length\", \"total_token_length\", \"start_time\",\n        \"end_time\", \"total_time\", \"model_name\", \"sample_input_limit\", \"errors\", \"created_at\", \"updated_at\"\n    ])\n\n    # Apply transformations\n    #metadata_df = metadata_df.with_column(\"output_response\", parse_json(col(\"output_response\")))\n    metadata_df = metadata_df.with_column(\"output_response\", col(\"output_response\").cast(\"string\"))\n    metadata_df = metadata_df.with_column(\"start_time\", col(\"start_time\").cast(\"number(38,0)\"))\n    metadata_df = metadata_df.with_column(\"end_time\", col(\"end_time\").cast(\"number(38,0)\"))\n    metadata_df = metadata_df.with_column(\"total_time\", col(\"total_time\").cast(\"float\"))\n    metadata_df = metadata_df.with_column(\"created_at\", current_timestamp())\n    metadata_df = metadata_df.with_column(\"updated_at\", current_timestamp())\n\n    try:\n        # Insert the data into the table\n        metadata_df.write.save_as_table(\"AI_MAPPING_TOOL_METADATA\", mode=\"append\")\n        \n        # Fetch the last inserted ID\n        last_id_df = session.sql(\"SELECT MAX(id) as last_id FROM AI_MAPPING_TOOL_METADATA\").collect()\n        st.session_state.metadata_id = last_id_df[0]['LAST_ID']\n        \n        st.success(\"Metadata inserted successfully!\")\n    except Exception as e:\n        st.error(f\"Error inserting metadata: {str(e)}\")\n        st.error(f\"Metadata: {metadata_df.collect()}\")\n\n    # Print for debugging\n    print(\"Debugging metadata_df:\")\n    for row in metadata_df.collect():\n        print(\"\\nRow:\")\n        for field_name, field_value in row.asDict().items():\n            print(f\"  {field_name}: \", end=\"\")\n            if isinstance(field_value, (datetime, date)):\n                print(field_value.isoformat())\n            elif isinstance(field_value, str):\n                print(f\"'{field_value[:100]}...'\" if len(field_value) > 100 else f\"'{field_value}'\")\n            elif isinstance(field_value, (int, float)):\n                print(f\"{field_value}\")\n            elif field_value is None:\n                print(\"None\")\n            else:\n                print(f\"{type(field_value).__name__}({repr(field_value)[:100]}...)\")\n        print(\"-\" * 50)\n\n# Print data types for debugging\nprint(f\"start_time type: {type(st.session_state.start_time)}\")\nprint(f\"end_time type: {type(st.session_state.end_time)}\")\nprint(f\"start_time value: {st.session_state.start_time}\")\nprint(f\"end_time value: {st.session_state.end_time}\")\n\ndef update_metadata(user_liked, user_feedback):\n    update_query = \"\"\"\n    UPDATE AI_MAPPING_TOOL_METADATA\n    SET user_liked = ?,\n        user_feedback = ?,\n        updated_at = CURRENT_TIMESTAMP()\n    WHERE id = ?\n    \"\"\"\n    try:\n        session.sql(update_query).update([user_liked, user_feedback, st.session_state.metadata_id])\n        st.success(\"Feedback submitted successfully!\")\n    except Exception as e:\n        st.error(f\"Error updating metadata: {str(e)}\")\n\ndef get_ai_response(options):\n    try:\n        st.session_state.start_time = datetime.now()\n        # Prepare the prompt\n        prompt_content = st.session_state.prompt.replace(\"'\", \"''\")\n        \n        # Call the CORTEX.COMPLETE function\n        query = f\"\"\"\n        SELECT SNOWFLAKE.CORTEX.COMPLETE(\n            '{model}',\n            ARRAY_CONSTRUCT(OBJECT_CONSTRUCT('role', 'user', 'content', '{prompt_content}')),\n            OBJECT_CONSTRUCT('temperature', {options['temperature']}, 'max_tokens', {options['max_tokens']}, 'top_p', {options['top_p']})\n        ) AS response\n        \"\"\"\n        result = session.sql(query).collect()\n        \n        if result and len(result) > 0:\n            response_json = json.loads(result[0]['RESPONSE'])\n            st.session_state.ai_response = response_json\n            st.session_state.end_time =  datetime.now()\n            \n            print(\"LLM Response done\")\n            insert_metadata()\n        else:\n            st.error(\"No response received from the AI model.\")\n    except Exception as e:\n        error_msg = f\"An error occurred: {str(e)}\\n{traceback.format_exc()}\"\n        st.error(error_msg)\n        st.session_state.ai_response = \"Error: Unable to get AI response.\"\n        st.session_state.errors.append(error_msg)\n\nif st.button(\"Generate Prompt\"):\n    generate_prompt()\n    \n    # Calculate token count\n    try:\n        token_count_result = session.sql(f\"\"\"\n        SELECT SNOWFLAKE.CORTEX.COUNT_TOKENS(\n            '{model}',\n            '{st.session_state.prompt.replace(\"'\", \"''\")}'\n        ) AS token_count\n        \"\"\").collect()\n        \n        if token_count_result and len(token_count_result) > 0:\n            st.session_state.token_count = token_count_result[0]['TOKEN_COUNT']\n        else:\n            st.warning(\"Unable to calculate token count.\")\n    except Exception as e:\n        st.warning(f\"Error calculating token count: {str(e)}\")\n\nif st.session_state.prompt:\n    st.subheader(\"Generated Prompt:\")\n    st.text_area(\"Prompt\", st.session_state.prompt, height=300)\n    st.write(f\"Token Count: {st.session_state.token_count}\")\n    \n    st.subheader(\"Model Parameters\")\n    temperature = st.slider(\"Temperature\", 0.0, 1.0, 0.7, 0.1)\n    max_tokens = st.number_input(\"Max Tokens\", 1, 4096, min(st.session_state.token_count, 4096))\n    top_p = st.slider(\"Top P\", 0.0, 1.0, 1.0, 0.1)\n    \n    if st.button(\"Get AI Response\"):\n        options = {\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n            \"top_p\": top_p,\n        }\n        get_ai_response(options)\n\nif st.session_state.ai_response:\n    st.subheader(\"AI Model Response:\")\n    if isinstance(st.session_state.ai_response, dict):\n        if 'choices' in st.session_state.ai_response and len(st.session_state.ai_response['choices']) > 0:\n            message = st.session_state.ai_response['choices'][0].get('message', {}).get('content', '')\n            print(\"before LLM Markdown\")\n            st.markdown(message)\n            #insert_metadata()\n        \n        if 'usage' in st.session_state.ai_response:\n            usage = st.session_state.ai_response['usage']\n            st.write(\"Token Usage:\")\n            st.write(f\"- Completion tokens: {usage.get('completion_tokens', 'N/A')}\")\n            st.write(f\"- Prompt tokens: {usage.get('prompt_tokens', 'N/A')}\")\n            st.write(f\"- Total tokens: {usage.get('total_tokens', 'N/A')}\")\n        \n        if 'model' in st.session_state.ai_response:\n            st.write(f\"Model used: {st.session_state.ai_response['model']}\")\n    else:\n        st.write(st.session_state.ai_response)\n\n    # User feedback\n    col1, col2 = st.columns(2)\n    with col1:\n        liked = st.button(\"ðŸ‘ Like\")\n    with col2:\n        disliked = st.button(\"ðŸ‘Ž Dislike\")\n    \n    feedback = st.text_area(\"Additional Feedback (optional)\")\n\n    if liked or disliked or feedback:\n        try:\n            user_liked = True if liked else (False if disliked else None)\n            update_metadata(user_liked, feedback)\n        except Exception as e:\n            error_msg = f\"Error submitting feedback: {str(e)}\\n{traceback.format_exc()}\"\n            st.error(error_msg)\n            st.session_state.errors.append(error_msg)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83288d4b-f82d-41a7-92e3-3f1b60bcfd9b",
   "metadata": {
    "language": "sql",
    "name": "cell7",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM AI_MAPPING_TOOL_METADATA \n/*DELETE FROM AI_MAPPING_TOOL_METADATA;\nINSERT INTO AI_MAPPING_TOOL_METADATA (\n    user,\n    role,\n    database_name,\n    schema_name,\n    modernized_table_name,\n    legacy_table_name,\n    vector_embeddings_used,\n    sample_data_used,\n    input_prompt,\n    output_response,\n    user_liked,\n    user_feedback,\n    input_token_length,\n    output_token_length,\n    total_token_length,\n    start_time,\n    end_time,\n    total_time,\n    model_name,\n    sample_input_limit,\n    errors\n)\nSELECT\n    'JOHN_DOE',\n    'DATA_ANALYST',\n    'SMART_AI_MAPPER',\n    'SMART_AI_MAPPER_TOOL',\n    'Deposit_Accounts_Modernised',\n    'Deposit_Accounts_Legacy',\n    TRUE,\n    TRUE,\n    'Compare the fields between the Modernised and Legacy Deposit_Accounts tables. Analyze the sample data to identify discrepancies and provide a mapping between the two tables.',\n    OBJECT_CONSTRUCT('choices', ARRAY_CONSTRUCT(OBJECT_CONSTRUCT('message', OBJECT_CONSTRUCT('content', 'Based on the analysis of the Modernised and Legacy Deposit_Accounts tables, here are the mappings and discrepancies identified: 1. Customer_ID (Modernised) -> Cust_ID (Legacy) 2. Account_Number (Modernised) -> Acct_Num (Legacy) 3. Account_Type (Modernised) -> Acct_Type (Legacy) 4. Balance (Modernised) -> Acct_Balance (Legacy) 5. Open_Date (Modernised) -> Opening_Date (Legacy) Discrepancies: 1. The Modernised table uses Customer_ID while the Legacy table uses Cust_ID for the unique identifier. 2. Account numbers are stored in Account_Number in the Modernised table and Acct_Num in the Legacy table. 3. The Balance field is named differently in both tables. 4. Date formats may differ between Open_Date and Opening_Date. Snowflake SQL to transform Modernised data to Legacy format: SELECT Customer_ID AS Cust_ID, Account_Number AS Acct_Num, Account_Type AS Acct_Type, Balance AS Acct_Balance, Open_Date AS Opening_Date FROM Deposit_Accounts_Modernised; Note: Ensure that the data types are compatible between the two tables, especially for the date and numeric fields. You may need to add appropriate CAST functions if there are any data type mismatches.'))), 'usage', OBJECT_CONSTRUCT('completion_tokens', 231, 'prompt_tokens', 124, 'total_tokens', 355)),\n    TRUE,\n    'The mapping provided is accurate and helpful.',\n    124,\n    231,\n    355,\n    '2023-08-06 10:15:30'::TIMESTAMP_NTZ,\n    '2023-08-06 10:15:35'::TIMESTAMP_NTZ,\n    5.0,\n    'snowflake-arctic',\n    5,\n    NULL\nUNION ALL\nSELECT\n    'JANE_SMITH',\n    'DATA_ENGINEER',\n    'SMART_AI_MAPPER',\n    'SMART_AI_MAPPER_TOOL',\n    'Customer_Info_Modernised',\n    'Customer_Info_Legacy',\n    FALSE,\n    TRUE,\n    'Provide a mapping between the Modernised and Legacy Customer_Info tables. Highlight any differences in field names or data values.',\n    OBJECT_CONSTRUCT('choices', ARRAY_CONSTRUCT(OBJECT_CONSTRUCT('message', OBJECT_CONSTRUCT('content', 'Based on the sample data provided for the Modernised and Legacy Customer_Info tables, here is the mapping and analysis: 1. CustomerID (Modernised) -> Cust_ID (Legacy) 2. FirstName (Modernised) -> First_Name (Legacy) 3. LastName (Modernised) -> Last_Name (Legacy) 4. Email (Modernised) -> EmailAddress (Legacy) 5. PhoneNumber (Modernised) -> Phone (Legacy) 6. DateOfBirth (Modernised) -> DOB (Legacy) Differences and Discrepancies: 1. The Modernised table uses camelCase for field names, while the Legacy table uses underscores. 2. The email field is named differently in both tables. 3. Phone number field has a different name in each table. 4. Date of Birth is abbreviated in the Legacy table. Snowflake SQL to transform Modernised data to Legacy format: SELECT CustomerID AS Cust_ID, FirstName AS First_Name, LastName AS Last_Name, Email AS EmailAddress, PhoneNumber AS Phone, DateOfBirth AS DOB FROM Customer_Info_Modernised; Note: Ensure that the data types are consistent between the two tables, especially for the Date of Birth field. You may need to use appropriate date formatting functions if the date formats differ between the tables.'))), 'usage', OBJECT_CONSTRUCT('completion_tokens', 246, 'prompt_tokens', 98, 'total_tokens', 344)),\n    TRUE,\n    'The mapping is correct and the SQL transformation is useful.',\n    98,\n    246,\n    344,\n    '2023-08-07 14:30:00'::TIMESTAMP_NTZ,\n    '2023-08-07 14:30:07'::TIMESTAMP_NTZ,\n    7.0,\n    'mistral-large',\n    10,\n    NULL\nUNION ALL\nSELECT\n    'BOB_JOHNSON',\n    'DATA_SCIENTIST',\n    'SMART_AI_MAPPER',\n    'SMART_AI_MAPPER_TOOL',\n    'Transaction_Modernised',\n    'Transaction_Legacy',\n    TRUE,\n    TRUE,\n    'Compare the Transaction_Modernised and Transaction_Legacy tables. Provide a mapping and suggest any necessary data transformations.',\n    OBJECT_CONSTRUCT('error', 'API request failed due to rate limiting. Please try again later.'),\n    FALSE,\n    'The AI model failed to provide a response due to rate limiting.',\n    115,\n    NULL,\n    NULL,\n    '2023-08-08 09:45:00'::TIMESTAMP_NTZ,\n    '2023-08-08 09:45:02'::TIMESTAMP_NTZ,\n    2.0,\n    'reka-flash',\n    5,\n    'API request failed due to rate limiting. Please try again later.';*/",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "882e0b15-9e4d-4ab9-80d0-bae28b9b032d",
   "metadata": {
    "language": "sql",
    "name": "cell8",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM AI_MAPPING_TOOL_METADATA",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "71372bfa-137d-4e37-82f6-2fac267bb316",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "collapsed": false
   },
   "outputs": [],
   "source": "import json\nfrom datetime import datetime, timedelta\nfrom snowflake.snowpark.functions import col, lit, current_timestamp\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\n\ndef test_insert_metadata(session, user, role, database_name, schema_name, modern_table_name, legacy_table_name,\n                         use_vector_embeddings, use_sample_data, model, record_limit, mock_ai_response):\n    logging.info(\"Starting metadata insertion test\")\n    \n    mock_start_time = datetime.now()\n    mock_end_time = mock_start_time + timedelta(seconds=5)  # Simulating 5 seconds of processing time\n    total_time = (mock_end_time - mock_start_time).total_seconds()\n\n    # Create a DataFrame with a single row of metadata\n    metadata_df = session.create_dataframe([\n        (None, user, role, database_name, schema_name, modern_table_name, legacy_table_name,\n         use_vector_embeddings, use_sample_data, \"Test prompt\", json.dumps(mock_ai_response), None, None,\n         150, mock_ai_response['usage']['completion_tokens'],\n         mock_ai_response['usage']['total_tokens'],\n         int(mock_start_time.timestamp()),\n         int(mock_end_time.timestamp()),\n         float(total_time), model, record_limit, None, None, None)\n    ], schema=[\n        \"id\", \"user\", \"role\", \"database_name\", \"schema_name\", \"modernized_table_name\", \"legacy_table_name\",\n        \"vector_embeddings_used\", \"sample_data_used\", \"input_prompt\", \"output_response\", \"user_liked\",\n        \"user_feedback\", \"input_token_length\", \"output_token_length\", \"total_token_length\", \"start_time\",\n        \"end_time\", \"total_time\", \"model_name\", \"sample_input_limit\", \"errors\", \"created_at\", \"updated_at\"\n    ])\n\n    # Apply transformations\n    metadata_df = metadata_df.select(\n        col(\"id\").cast(\"integer\"),\n        col(\"output_response\").cast(\"string\"),\n        col(\"start_time\").cast(\"number(38,0)\"),\n        col(\"end_time\").cast(\"number(38,0)\"),\n        col(\"total_time\").cast(\"float\"),\n        col(\"input_token_length\").cast(\"integer\"),\n        col(\"output_token_length\").cast(\"integer\"),\n        col(\"total_token_length\").cast(\"integer\"),\n        col(\"sample_input_limit\").cast(\"integer\"),\n        col(\"model_name\").cast(\"string\"),\n        col(\"vector_embeddings_used\").cast(\"boolean\"),\n        col(\"sample_data_used\").cast(\"boolean\"),\n        current_timestamp().alias(\"created_at\"),\n        current_timestamp().alias(\"updated_at\"),\n        \"*\"\n    )\n\n    try:\n        # Insert the data into the table\n        metadata_df.write.save_as_table(\"AI_MAPPING_TOOL_METADATA\", mode=\"append\")\n        \n        # Fetch the last inserted ID\n        last_id_df = session.sql(\"SELECT MAX(id) as last_id FROM AI_MAPPING_TOOL_METADATA\").collect()\n        last_inserted_id = last_id_df[0]['LAST_ID']\n        \n        logging.info(f\"Test metadata inserted successfully! Last inserted ID: {last_inserted_id}\")\n        \n        # Verify the inserted data\n        inserted_data = session.sql(f\"SELECT * FROM AI_MAPPING_TOOL_METADATA WHERE id = {last_inserted_id}\").collect()\n        assert len(inserted_data) == 1, \"Inserted data not found\"\n        \n        return last_inserted_id\n    except Exception as e:\n        logging.error(f\"Error inserting test metadata: {str(e)}\")\n        logging.debug(f\"Test Metadata: {metadata_df.collect()}\")\n        raise\n    finally:\n        logging.info(\"DataFrame Schema:\")\n        metadata_df.printSchema()\n\n# Stored procedure handler\ndef main(session):\n    try:\n        # Mock AI response\n        mock_ai_response = {\n            \"usage\": {\n                \"completion_tokens\": 100,\n                \"total_tokens\": 250\n            }\n        }\n\n        # Call the test function\n        last_id = test_insert_metadata(\n            session=session,\n            user=\"test_user\",\n            role=\"test_role\",\n            database_name=\"test_db\",\n            schema_name=\"test_schema\",\n            modern_table_name=\"modern_table\",\n            legacy_table_name=\"legacy_table\",\n            use_vector_embeddings=True,\n            use_sample_data=True,\n            model=\"test_model\",\n            record_limit=1000,\n            mock_ai_response=mock_ai_response\n        )\n        return f\"Test completed successfully. Last inserted ID: {last_id}\"\n\n    except Exception as e:\n        return f\"Test failed: {str(e)}\"\n\n# This is needed for Snowflake to recognize the stored procedure\ndef run(session):\n    return main(session)",
   "execution_count": null
  }
 ]
}